{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join as pjoin\n",
    "import os, cv2\n",
    "import re\n",
    "import random, tqdm\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchinfo\n",
    "from  torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchmetrics.classification import MulticlassAccuracy, MulticlassF1Score, MulticlassPrecision, MulticlassRecall\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------------------\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import albumentations as album\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch import utils"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data/tiff/'\n",
    "\n",
    "x_train_dir = os.path.join(DATA_DIR, 'train')\n",
    "y_train_dir = os.path.join(DATA_DIR, 'train_labels')\n",
    "\n",
    "x_valid_dir = os.path.join(DATA_DIR, 'val')\n",
    "y_valid_dir = os.path.join(DATA_DIR, 'val_labels')\n",
    "\n",
    "x_test_dir = os.path.join(DATA_DIR, 'test')\n",
    "y_test_dir = os.path.join(DATA_DIR, 'test_labels')\n",
    "\n",
    "class_dict = pd.read_csv(\"data/label_class_dict.csv\")\n",
    "# Классы\n",
    "class_names = class_dict['name'].tolist()\n",
    "# Классы RGB значений\n",
    "class_rgb_values = class_dict[['r','g','b']].values.tolist()\n",
    "\n",
    "print('Все классы набора данных и соответствующие им значения RGB в метках')\n",
    "print('Классы: ', class_names)\n",
    "print('Классы RGB значений: ', class_rgb_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Useful to shortlist specific classes in datasets with large number of classes\n",
    "select_classes = ['background', 'road']\n",
    "\n",
    "# Get RGB values of required classes\n",
    "select_class_indices = [class_names.index(cls.lower()) for cls in select_classes]\n",
    "select_class_rgb_values =  np.array(class_rgb_values)[select_class_indices]\n",
    "\n",
    "print('Selected classes and their corresponding RGB values in labels:')\n",
    "print('Class Names: ', class_names)\n",
    "print('Class RGB values: ', class_rgb_values)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for data visualization -------\n",
    "def visualize(**images):\n",
    "    \"\"\"\n",
    "    Построение изображений в один ряд\n",
    "    \"\"\"\n",
    "    n_images = len(images)\n",
    "    plt.figure(figsize=(20, 8))\n",
    "    for idx, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n_images, idx + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        # get title from the parameter names\n",
    "        plt.title(name.replace('_', ' ').title(), fontsize=20)\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "# Perform one hot encoding on label\n",
    "def one_hot_encode(label, label_values):\n",
    "    \"\"\"\n",
    "    Преобразуйте массив меток изображения с сегментацией в формат one-hot\n",
    "    , заменив значение каждого пикселя вектором длины num_classes\n",
    "    # Arguments\n",
    "    label: Метка изображения сегментации 2D-массива\n",
    "    label_values\n",
    "\n",
    "    # return\n",
    "    2D-массив с той же шириной и высотой, что и входные данные, но\n",
    "    с размером глубины num_classes\n",
    "    \"\"\"\n",
    "    semantic_map = []\n",
    "    for colour in label_values:\n",
    "        equality = np.equal(label, colour)\n",
    "        class_map = np.all(equality, axis=-1)\n",
    "        semantic_map.append(class_map)\n",
    "    semantic_map = np.stack(semantic_map, axis=-1)\n",
    "\n",
    "    return semantic_map\n",
    "\n",
    "# Perform reverse one-hot-encoding on labels / preds\n",
    "\n",
    "\n",
    "def reverse_one_hot(image):\n",
    "    \"\"\"\n",
    "    Преобразуйте 2D-массив в одноразовом формате (глубина равна num_classes)\n",
    "    в 2D-массив только с 1 каналом, где каждое значение пикселя является\n",
    "    ключом классифицированного класса.\n",
    "    Изображение # Arguments\n",
    "    : Изображение в формате one-hot \n",
    "\n",
    "    # return\n",
    "    2D-массив с той же шириной и высотой, что и входные данные, но\n",
    "    с размером глубины 1, где каждое значение пикселя является ключом классифицированного\n",
    "    класса.\n",
    "    \"\"\"\n",
    "    x = np.argmax(image, axis=-1)\n",
    "    return x\n",
    "\n",
    "# Perform colour coding on the reverse-one-hot outputs\n",
    "\n",
    "\n",
    "def colour_code_segmentation(image, label_values):\n",
    "    \"\"\"\n",
    "    Учитывая 1-канальный массив ключей класса, цветовое кодирование результатов сегментации.\n",
    "    #\n",
    "    Изображение аргументов: одноканальный массив, где каждое значение представляет ключ класса.\n",
    "    label_values (значения метки)\n",
    "\n",
    "    # return\n",
    "    Изображение с цветовой кодировкой для визуализации сегментации\n",
    "    \"\"\"\n",
    "    colour_codes = np.array(label_values)\n",
    "    x = colour_codes[image.astype(int)]\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoadsDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    \"\"\"Набор данных о дорогах Массачусетса. Считывайте изображения, применяйте увеличения и преобразования предварительной обработки.\n",
    "    \n",
    "    Args:\n",
    "        images_dir (str): путь к папке\n",
    "        masks_dir (str): путь к папке масок сегментации\n",
    "        class_rgb_values (список): значения RGB выбранных классов для извлечения из\n",
    "        augmentation: конвейер преобразования данных\n",
    "        (например, переворачивание, масштабирование и т.д.),\n",
    "        preprocessing: предварительная обработка данных\n",
    "        (например, нормализация, манипулирование формой и т.д.)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "            self, \n",
    "            images_dir, \n",
    "            masks_dir, \n",
    "            class_rgb_values=None, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        \n",
    "        self.image_paths = [os.path.join(images_dir, image_id) for image_id in sorted(os.listdir(images_dir))]\n",
    "        self.mask_paths = [os.path.join(masks_dir, image_id) for image_id in sorted(os.listdir(masks_dir))]\n",
    "\n",
    "        self.class_rgb_values = class_rgb_values\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \n",
    "        image = cv2.cvtColor(cv2.imread(self.image_paths[i]), cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.cvtColor(cv2.imread(self.mask_paths[i]), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        mask = one_hot_encode(mask, self.class_rgb_values).astype('float')\n",
    "        \n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "            \n",
    "        return image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = RoadsDataset(x_train_dir, y_train_dir, class_rgb_values=select_class_rgb_values)\n",
    "random_idx = random.randint(0, len(dataset)-1)\n",
    "image, mask = dataset[random_idx]\n",
    "\n",
    "visualize(\n",
    "    original_image = image,\n",
    "    ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n",
    "    one_hot_encoded_mask = reverse_one_hot(mask)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NORMALIZE_MEAN_IMG = [0.4295, 0.4325, 0.3961]\n",
    "NORMALIZE_DEVIATIONS_IMG = [0.2267, 0.2192, 0.2240]\n",
    "\n",
    "def get_training_augmentation():\n",
    "    train_transform = [    \n",
    "        album.RandomCrop(height=256, width=256, always_apply=True),\n",
    "        album.OneOf(\n",
    "            [\n",
    "                album.HorizontalFlip(p=1),\n",
    "                album.VerticalFlip(p=1),\n",
    "                album.RandomRotate90(p=1),\n",
    "            ],\n",
    "            p=0.75,\n",
    "        ),\n",
    "        # album.Normalize(mean=NORMALIZE_MEAN_IMG, std=NORMALIZE_DEVIATIONS_IMG, always_apply=True)\n",
    "    ]\n",
    "    return album.Compose(train_transform)\n",
    "\n",
    "\n",
    "def get_validation_augmentation():   \n",
    "    # Add sufficient padding to ensure image is divisible by 32\n",
    "    test_transform = [\n",
    "        album.PadIfNeeded(min_height=1536, min_width=1536, always_apply=True, border_mode=0),\n",
    "    ]\n",
    "    return album.Compose(test_transform)\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "def get_preprocessing(preprocessing_fn=None):\n",
    "    \"\"\"Construct preprocessing transform    \n",
    "    Args:\n",
    "        preprocessing_fn (callable): data normalization function \n",
    "            (can be specific for each pretrained neural network)\n",
    "    Return:\n",
    "        transform: albumentations.Compose\n",
    "    \"\"\"   \n",
    "    _transform = []\n",
    "    if preprocessing_fn:\n",
    "        _transform.append(album.Lambda(image=preprocessing_fn))\n",
    "    _transform.append(album.Lambda(image=to_tensor, mask=to_tensor))\n",
    "        \n",
    "    return album.Compose(_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_dataset = RoadsDataset(\n",
    "    x_train_dir, y_train_dir, \n",
    "    augmentation=get_training_augmentation(),\n",
    "    class_rgb_values=select_class_rgb_values,\n",
    ")\n",
    "\n",
    "random_idx = random.randint(0, len(augmented_dataset)-1)\n",
    "\n",
    "# Different augmentations on a random image/mask pair (256*256 crop)\n",
    "for i in range(3):\n",
    "    image, mask = augmented_dataset[random_idx]\n",
    "    visualize(\n",
    "        original_image = image,\n",
    "        ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n",
    "        one_hot_encoded_mask = reverse_one_hot(mask)\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Треня"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER = 'resnet50'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "CLASSES = select_classes\n",
    "ACTIVATION = nn.ReLU # could be None for logits or 'softmax2d' for multiclass segmentation\n",
    "\n",
    "# create segmentation model with pretrained encoder\n",
    "model = smp.Unet(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=len(CLASSES), \n",
    "    activation=ACTIVATION,\n",
    ")\n",
    "\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train and val dataset instances\n",
    "train_dataset = RoadsDataset(\n",
    "    x_train_dir, y_train_dir, \n",
    "    augmentation=get_training_augmentation(),\n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    class_rgb_values=select_class_rgb_values,\n",
    ")\n",
    "\n",
    "valid_dataset = RoadsDataset(\n",
    "    x_valid_dir, y_valid_dir, \n",
    "    augmentation=get_validation_augmentation(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    class_rgb_values=select_class_rgb_values,\n",
    ")\n",
    "\n",
    "# Get train and val data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set flag to train the model or not. If set to 'False', only prediction is performed (using an older model checkpoint)\n",
    "TRAINING = True\n",
    "\n",
    "# Set num of epochs\n",
    "EPOCHS = 5\n",
    "\n",
    "# Set device: `cuda` or `cpu`\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# define loss function\n",
    "loss = smp.utils.losses.DiceLoss()\n",
    "\n",
    "# define metrics\n",
    "metrics = [\n",
    "    smp.utils.metrics.IoU(threshold=0.5),\n",
    "]\n",
    "\n",
    "# define optimizer\n",
    "optimizer = torch.optim.Adam([ \n",
    "    dict(params=model.parameters(), lr=0.00008),\n",
    "])\n",
    "\n",
    "# define learning rate scheduler (not used in this NB)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "    optimizer, T_0=1, T_mult=2, eta_min=5e-5,\n",
    ")\n",
    "\n",
    "# load best saved model checkpoint from previous commit (if present)\n",
    "if os.path.exists('../input/unet-resnet50-frontend-road-segmentation-pytorch/best_model.pth'):\n",
    "    model = torch.load('../input/unet-resnet50-frontend-road-segmentation-pytorch/best_model.pth', map_location=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_epoch = smp.utils.train.TrainEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    optimizer=optimizer,\n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_epoch = smp.utils.train.ValidEpoch(\n",
    "    model, \n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if TRAINING:\n",
    "\n",
    "    best_iou_score = 0.0\n",
    "    train_logs_list, valid_logs_list = [], []\n",
    "\n",
    "    for i in range(0, EPOCHS):\n",
    "\n",
    "        # Perform training & validation\n",
    "        print('\\nEpoch: {}'.format(i))\n",
    "        train_logs = train_epoch.run(train_loader)\n",
    "        print('\\train_logs: {}'.format(i))\n",
    "        valid_logs = valid_epoch.run(valid_loader)\n",
    "        print('\\valid_logs: {}'.format(i))\n",
    "        train_logs_list.append(train_logs)\n",
    "        valid_logs_list.append(valid_logs)\n",
    "\n",
    "        # Save model if a better val IoU score is obtained\n",
    "        if best_iou_score < valid_logs['iou_score']:\n",
    "            best_iou_score = valid_logs['iou_score']\n",
    "            torch.save(model, './best_model.pth')\n",
    "            print('Model saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best saved model checkpoint from the current run\n",
    "if os.path.exists('./best_model.pth'):\n",
    "    best_model = torch.load('./best_model.pth', map_location=DEVICE)\n",
    "    print('Loaded UNet model from this run.')\n",
    "\n",
    "# load best saved model checkpoint from previous commit (if present)\n",
    "elif os.path.exists('../input/unet-resnet50-frontend-road-segmentation-pytorch/best_model.pth'):\n",
    "    best_model = torch.load('../input/unet-resnet50-frontend-road-segmentation-pytorch/best_model.pth', map_location=DEVICE)\n",
    "    print('Loaded UNet model from a previous commit.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create test dataloader to be used with UNet model (with preprocessing operation: to_tensor(...))\n",
    "test_dataset = RoadsDataset(\n",
    "    x_test_dir, \n",
    "    y_test_dir, \n",
    "    augmentation=get_validation_augmentation(), \n",
    "    preprocessing=get_preprocessing(preprocessing_fn),\n",
    "    class_rgb_values=select_class_rgb_values,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(test_dataset)\n",
    "\n",
    "# test dataset for visualization (without preprocessing transformations)\n",
    "test_dataset_vis = RoadsDataset(\n",
    "    x_test_dir, y_test_dir, \n",
    "    augmentation=get_validation_augmentation(),\n",
    "    class_rgb_values=select_class_rgb_values,\n",
    ")\n",
    "\n",
    "# get a random test image/mask index\n",
    "random_idx = random.randint(0, len(test_dataset_vis)-1)\n",
    "image, mask = test_dataset_vis[random_idx]\n",
    "\n",
    "visualize(\n",
    "    original_image = image,\n",
    "    ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), select_class_rgb_values),\n",
    "    one_hot_encoded_mask = reverse_one_hot(mask)\n",
    ")\n",
    "\n",
    "# Notice the images / masks are 1536*1536 because of 18px padding on all sides. \n",
    "# This is to ensure the input image dimensions to UNet model are a multiple of 2 (to account for pooling & transpose conv. operations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Center crop padded image / mask to original image dims\n",
    "def crop_image(image, target_image_dims=[1500,1500,3]):\n",
    "   \n",
    "    target_size = target_image_dims[0]\n",
    "    image_size = len(image)\n",
    "    padding = (image_size - target_size) // 2\n",
    "\n",
    "    if padding<0:\n",
    "        return image\n",
    "\n",
    "    return image[\n",
    "        padding:image_size - padding,\n",
    "        padding:image_size - padding,\n",
    "        :,\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_preds_folder = 'sample_predictions/'\n",
    "if not os.path.exists(sample_preds_folder):\n",
    "    os.makedirs(sample_preds_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in range(len(test_dataset)):\n",
    "\n",
    "    image, gt_mask = test_dataset[idx]\n",
    "    image_vis = crop_image(test_dataset_vis[idx][0].astype('uint8'))\n",
    "    x_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n",
    "    # Predict test image\n",
    "    pred_mask = best_model(x_tensor)\n",
    "    pred_mask = pred_mask.detach().squeeze().cpu().numpy()\n",
    "    # Convert pred_mask from `CHW` format to `HWC` format\n",
    "    pred_mask = np.transpose(pred_mask,(1,2,0))\n",
    "    # Get prediction channel corresponding to road\n",
    "    pred_road_heatmap = pred_mask[:,:,select_classes.index('road')]\n",
    "    pred_mask = crop_image(colour_code_segmentation(reverse_one_hot(pred_mask), select_class_rgb_values))\n",
    "    # Convert gt_mask from `CHW` format to `HWC` format\n",
    "    gt_mask = np.transpose(gt_mask,(1,2,0))\n",
    "    gt_mask = crop_image(colour_code_segmentation(reverse_one_hot(gt_mask), select_class_rgb_values))\n",
    "    cv2.imwrite(os.path.join(sample_preds_folder, f\"sample_pred_{idx}.png\"), np.hstack([image_vis, gt_mask, pred_mask])[:,:,::-1])\n",
    "    \n",
    "    visualize(\n",
    "        original_image = image_vis,\n",
    "        ground_truth_mask = gt_mask,\n",
    "        predicted_mask = pred_mask,\n",
    "        predicted_road_heatmap = pred_road_heatmap\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_epoch = smp.utils.train.ValidEpoch(\n",
    "    model,\n",
    "    loss=loss, \n",
    "    metrics=metrics, \n",
    "    device=DEVICE,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "valid_logs = test_epoch.run(test_dataloader)\n",
    "print(\"Evaluation on Test Data: \")\n",
    "print(f\"Mean IoU Score: {valid_logs['iou_score']:.4f}\")\n",
    "print(f\"Mean Dice Loss: {valid_logs['dice_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_logs_df = pd.DataFrame(train_logs_list)\n",
    "valid_logs_df = pd.DataFrame(valid_logs_list)\n",
    "train_logs_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(train_logs_df.index.tolist(), train_logs_df.iou_score.tolist(), lw=3, label = 'Train')\n",
    "plt.plot(valid_logs_df.index.tolist(), valid_logs_df.iou_score.tolist(), lw=3, label = 'Valid')\n",
    "plt.xlabel('Epochs', fontsize=21)\n",
    "plt.ylabel('IoU Score', fontsize=21)\n",
    "plt.title('IoU Score Plot', fontsize=21)\n",
    "plt.legend(loc='best', fontsize=16)\n",
    "plt.grid()\n",
    "plt.savefig('iou_score_plot.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,8))\n",
    "plt.plot(train_logs_df.index.tolist(), train_logs_df.dice_loss.tolist(), lw=3, label = 'Train')\n",
    "plt.plot(valid_logs_df.index.tolist(), valid_logs_df.dice_loss.tolist(), lw=3, label = 'Valid')\n",
    "plt.xlabel('Epochs', fontsize=21)\n",
    "plt.ylabel('Dice Loss', fontsize=21)\n",
    "plt.title('Dice Loss Plot', fontsize=21)\n",
    "plt.legend(loc='best', fontsize=16)\n",
    "plt.grid()\n",
    "plt.savefig('dice_loss_plot.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
