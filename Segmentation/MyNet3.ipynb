{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# массивы, рандом\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# файлы\n",
    "from os.path import join as pjoin\n",
    "import os\n",
    "import json\n",
    "\n",
    "# модельки\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# аугментация\n",
    "import albumentations as album\n",
    "\n",
    "# отображение\n",
    "# from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchinfo\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from board import uniqufy_path, create_image_plot\n",
    "\n",
    "# метрики\n",
    "from torchmetrics.classification import BinaryJaccardIndex\n",
    "import torchmetrics.classification as metrics\n",
    "\n",
    "# оптимизаторы, изменение lr\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\n",
    "\n",
    "# лос\n",
    "from segmentation_models_pytorch.losses import LovaszLoss, DiceLoss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Константы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data/tiff/'\n",
    "DATA_CLASSES = \"./data/label_class_dict.csv\"\n",
    "\n",
    "MEAN_IMAGE_TRANSFORM = [0.4363, 0.4328, 0.3291]\n",
    "MEAN_IMAGE_STD = [0.2129, 0.2075, 0.2038]\n",
    "SIZE_IMAGE = 256\n",
    "BATCH_SIZE = 16\n",
    "NUM_WORKERS = 0\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Классы:  ['background', 'road']\n",
      "Классы RGB значений:  [[0, 0, 0], [255, 255, 255]]\n"
     ]
    }
   ],
   "source": [
    "x_train_dir = os.path.join(DATA_DIR, 'train')\n",
    "y_train_dir = os.path.join(DATA_DIR, 'train_labels')\n",
    "\n",
    "x_valid_dir = os.path.join(DATA_DIR, 'val')\n",
    "y_valid_dir = os.path.join(DATA_DIR, 'val_labels')\n",
    "\n",
    "x_test_dir = os.path.join(DATA_DIR, 'test')\n",
    "y_test_dir = os.path.join(DATA_DIR, 'test_labels')\n",
    "\n",
    "class_dict = pd.read_csv(\"./data/label_class_dict.csv\")\n",
    "CLASSES = class_dict['name'].tolist()\n",
    "CLASSES_RGB = class_dict[['r','g','b']].values.tolist()\n",
    "\n",
    "print('Классы: ', CLASSES)\n",
    "print('Классы RGB значений: ', CLASSES_RGB)\n",
    "\n",
    "with open('parametrs.json', 'r', encoding='utf-8') as f:\n",
    "    PARAMETERS = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОТОБРАЖЕНИЕ\n",
    "def print_image(**images):\n",
    "    n_images = len(images)\n",
    "    plt.figure(figsize=(16, 4))\n",
    "    for idx, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n_images, idx + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(name.replace('_', ' ').title(), fontsize=20)\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "# ПРЕОБРАЗОВАНИЯ\n",
    "def one_hot_encode(label, label_values):\n",
    "    semantic_map = []\n",
    "    for colour in label_values:\n",
    "        equality = np.equal(label, colour)\n",
    "        class_map = np.all(equality, axis=-1)\n",
    "        semantic_map.append(class_map)\n",
    "    semantic_map = np.stack(semantic_map, axis=-1)\n",
    "    return semantic_map\n",
    "\n",
    "\n",
    "def reverse_one_hot(image):\n",
    "    x = np.argmax(image, axis=-1)\n",
    "    return x\n",
    "\n",
    "\n",
    "def colour_code_segmentation(image, label_values):\n",
    "    colour_codes = np.array(label_values)\n",
    "    x = colour_codes[image.astype(int)]\n",
    "    return x\n",
    "\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "\n",
    "def prepare_to_network():\n",
    "    return album.Lambda(image=to_tensor, mask=to_tensor)\n",
    "\n",
    "\n",
    "def transform_train():\n",
    "    train_transform = album.Compose(\n",
    "        [\n",
    "            album.OneOf(\n",
    "                [\n",
    "                    album.HorizontalFlip(p=1),\n",
    "                    album.VerticalFlip(p=1),\n",
    "                    album.RandomRotate90(p=1),\n",
    "                ],\n",
    "                p=0.75,\n",
    "            ),\n",
    "            album.Normalize(mean=MEAN_IMAGE_TRANSFORM,\n",
    "                            std=MEAN_IMAGE_STD, always_apply=True)\n",
    "        ]\n",
    "    )\n",
    "    return train_transform\n",
    "\n",
    "\n",
    "def transform_test():\n",
    "    test_transform = album.Compose([\n",
    "        album.Normalize(mean=MEAN_IMAGE_TRANSFORM,\n",
    "                          std=MEAN_IMAGE_STD, always_apply=True)\n",
    "    ])\n",
    "    return test_transform\n",
    "\n",
    "# ПОЛУЧЕНИЕ ПАРАМЕТРОВ ДЛЯ ОБУЧЕНИЯ\n",
    "\n",
    "\n",
    "def get_model(param, encoder, encoder_weights, classes, activation):\n",
    "    if (param == 'unet'):\n",
    "        return smp.Unet(\n",
    "            encoder_name=encoder,\n",
    "            encoder_weights=encoder_weights,\n",
    "            classes=len(classes),\n",
    "            activation=activation,\n",
    "        )\n",
    "\n",
    "    return smp.Unet(\n",
    "        encoder_name=encoder,\n",
    "        encoder_weights=encoder_weights,\n",
    "        classes=len(classes),\n",
    "        activation=activation,)\n",
    "\n",
    "\n",
    "def get_function_loss(param):\n",
    "    if (param == 'DiceLoss'):\n",
    "        return DiceLoss(mode='binary')\n",
    "\n",
    "    return LovaszLoss(mode='binary')\n",
    "\n",
    "\n",
    "def get_optimizer(param, model, lr, weight_decay: float = 0.1, momentum: float = 0.9):\n",
    "    if param == 'Adam':\n",
    "        return torch.optim.Adam(\n",
    "            params=model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    return torch.optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "\n",
    "\n",
    "def get_scheduler(param, optimizer):\n",
    "    if param['name'] == 'ReduceLROnPlateau':\n",
    "        return ReduceLROnPlateau(optimizer, 'min',\n",
    "                                 patience=param['patience'], threshold=param['threshold'],\n",
    "                                 cooldown=param['cooldown'], factor=param['factor'])\n",
    "\n",
    "    return CosineAnnealingWarmRestarts(\n",
    "        optimizer, T_0=1, T_mult=2, eta_min=5e-5,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "def get_metric(param, iou, acc, dice):\n",
    "    if param == 'iou':\n",
    "        return iou\n",
    "    if param == 'acc':\n",
    "        return acc\n",
    "    if param == 'dice':\n",
    "        return dice\n",
    "    return 0\n",
    "\n",
    "def saveDivide(x, y): return torch.nan_to_num(x/y)\n",
    "\n",
    "def calculate_metric(numerator, denominator, classes):\n",
    "    with torch.no_grad():\n",
    "        metric_values = saveDivide(numerator, denominator)\n",
    "        metric_per_class = {classname: val.item()\n",
    "                            for classname, val in zip(classes, metric_values)}\n",
    "        metric_average = torch.sum(metric_values)/len(classes)\n",
    "        metric_average_micro = saveDivide(torch.sum(numerator), torch.sum(denominator))\n",
    "        return (metric_values, metric_per_class, metric_average, metric_average_micro)\n",
    "\n",
    "\n",
    "# def dice_score(pred: torch.Tensor, mask: torch.Tensor):\n",
    "#     intersection = torch.sum(pred * mask)\n",
    "#     dice = (2. * intersection) / (torch.sum(pred) + torch.sum(mask))\n",
    "#     return dice.item()\n",
    "\n",
    "# def dice_coefficient_numpy(pred: torch.Tensor, target: torch.Tensor):\n",
    "#     smooth = 1e-7  # чтобы избежать деления на ноль\n",
    "#     pred = pred / 255.0  # нормализуем тензор предсказания\n",
    "#     target = target / 255.0  # нормализуем тензор маски\n",
    "#     pred_flat = pred.view(-1)\n",
    "#     target_flat = target.view(-1)\n",
    "#     intersection = torch.sum(pred_flat * target_flat)\n",
    "#     dice = (2. * intersection + smooth) / \\\n",
    "#         (torch.sum(pred_flat) + torch.sum(target_flat) + smooth)\n",
    "#     return dice\n",
    "\n",
    "# def pixel_accuracy(pred: torch.Tensor, mask: torch.Tensor):\n",
    "#     correct = torch.eq(pred, mask).int()\n",
    "#     return float(correct.sum()) / float(correct.numel())\n",
    "\n",
    "# САМО ОБУЧЕНИЕ\n",
    "def train_step(model, criterion, optimizer, dataloader, epoch, epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.\n",
    "\n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss\n",
    "\n",
    "    with torch.no_grad():\n",
    "        train_loss = running_loss / len(dataloader)\n",
    "    return train_loss.item()\n",
    "\n",
    "\n",
    "def valid_step(model, criterion, dataloader, epoch, BOARD):\n",
    "    model.eval()\n",
    "    running_loss = 0.\n",
    "\n",
    "    iou = BinaryJaccardIndex(num_classes=2)\n",
    "    iou.to(DEVICE)\n",
    "    METRIC = metrics.MulticlassStatScores(num_classes=len(CLASSES), average=None)\n",
    "    METRIC.to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(dataloader):\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            output = model(images)\n",
    "\n",
    "            iou(output, labels)\n",
    "            METRIC.update(output, labels)\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "            running_loss += loss\n",
    "\n",
    "            BOARD.add_figure('valid_sample_' + str(i), create_image_plot(\n",
    "                origin=images[0].cpu().numpy().transpose(2, 1, 0),\n",
    "                true=colour_code_segmentation(reverse_one_hot(\n",
    "                    labels[0].cpu().numpy().transpose(2, 1, 0)), CLASSES_RGB),\n",
    "                pred=colour_code_segmentation(reverse_one_hot(\n",
    "                    output[0].cpu().numpy().transpose(2, 1, 0)), CLASSES_RGB)),\n",
    "                epoch)\n",
    "\n",
    "        tp, fp, tn, fn = METRIC._final_state()\n",
    "        \n",
    "        jaccard = calculate_metric(tp, (tp+fp+fn), classes=CLASSES)\n",
    "        dice = calculate_metric(2*tp, 2*tp+tn+tp, classes=CLASSES)\n",
    "        acc = calculate_metric((tp+tn), (tp+fp+tn+fn), classes=CLASSES)\n",
    "\n",
    "        valid_loss = running_loss / len(dataloader)\n",
    "        return valid_loss.item(), iou.compute().item(), dice, acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoadsDataset(Dataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            images_dir, \n",
    "            masks_dir, \n",
    "            class_rgb_values=None, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        self.image_paths = [os.path.join(images_dir, image_id) for image_id in sorted(os.listdir(images_dir))]\n",
    "        self.mask_paths = [os.path.join(masks_dir, image_id) for image_id in sorted(os.listdir(masks_dir))]\n",
    "\n",
    "        self.class_rgb_values = class_rgb_values\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        image = cv2.cvtColor(cv2.imread(self.image_paths[i]), cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.cvtColor(cv2.imread(self.mask_paths[i]), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        mask = one_hot_encode(mask, self.class_rgb_values).astype('float')\n",
    "        \n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        return image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка валидации и теста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = RoadsDataset(x_valid_dir, y_valid_dir,\n",
    "                             class_rgb_values=CLASSES_RGB, augmentation=transform_test(), preprocessing=prepare_to_network())\n",
    "test_dataset = RoadsDataset(x_test_dir, y_test_dir,\n",
    "                            class_rgb_values=CLASSES_RGB, augmentation=transform_test(), preprocessing=prepare_to_network())\n",
    "\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=4,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=4,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ОБУЧЕНИЕ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 7\n",
    "NUM_WORKERS = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "635fc0db233e4d4bad0d43dfc44d36ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8588ec40982a423bb3f39fecc7b174d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 40\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[39mfor\u001b[39;00m i, param_group \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(optimizer\u001b[39m.\u001b[39mparam_groups):\n\u001b[0;32m     38\u001b[0m     BOARD\u001b[39m.\u001b[39madd_scalar(\u001b[39m'\u001b[39m\u001b[39mlearning rate\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mfloat\u001b[39m(param_group[\u001b[39m'\u001b[39m\u001b[39mlr\u001b[39m\u001b[39m'\u001b[39m]), epoch)\n\u001b[1;32m---> 40\u001b[0m train_loss \u001b[39m=\u001b[39m train_step(\n\u001b[0;32m     41\u001b[0m     model, criterion, optimizer, train_dataloader, epoch, EPOCHS)\n\u001b[0;32m     42\u001b[0m valid_loss, metric_iou, metric_dice, metric_acc \u001b[39m=\u001b[39m valid_step(\n\u001b[0;32m     43\u001b[0m     model, criterion, valid_dataloader, epoch, BOARD)\n\u001b[0;32m     45\u001b[0m scheduler\u001b[39m.\u001b[39mstep(valid_loss)\n",
      "Cell \u001b[1;32mIn[4], line 160\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(model, criterion, optimizer, dataloader, epoch, epochs)\u001b[0m\n\u001b[0;32m    157\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m    158\u001b[0m running_loss \u001b[39m=\u001b[39m \u001b[39m0.\u001b[39m\n\u001b[1;32m--> 160\u001b[0m \u001b[39mfor\u001b[39;00m i, (images, labels) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[0;32m    161\u001b[0m     images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[0;32m    162\u001b[0m     labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(DEVICE)\n",
      "File \u001b[1;32mc:\\Users\\emely\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\emely\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\emely\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\emely\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[5], line 18\u001b[0m, in \u001b[0;36mRoadsDataset.__getitem__\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, i):\n\u001b[1;32m---> 18\u001b[0m     image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(cv2\u001b[39m.\u001b[39;49mimread(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mimage_paths[i]), cv2\u001b[39m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m     19\u001b[0m     mask \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(cv2\u001b[39m.\u001b[39mimread(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmask_paths[i]), cv2\u001b[39m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m     21\u001b[0m     mask \u001b[39m=\u001b[39m one_hot_encode(mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclass_rgb_values)\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mfloat\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for (index_param, param) in tqdm(enumerate(PARAMETERS[\"paramerts\"])):\n",
    "    id = f\"{param['model']}_{param['encoder']}_{param['learning_rate']}_{param['metric']}_{param['optimizer']}_{param['scheduler']['name']}_{param['loss']}_{index_param}\"\n",
    "    TBpath = uniqufy_path('res/' + id)\n",
    "    BOARD = SummaryWriter(TBpath)\n",
    "\n",
    "    LEARNING_RATE = param['learning_rate']\n",
    "\n",
    "    ACTIVATION = nn.ReLU\n",
    "\n",
    "    # МОДЕЛЬ\n",
    "    model = get_model(param['model'], param['encoder'],\n",
    "                      'imagenet', CLASSES, ACTIVATION)\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    # ЗАГРУЗКА\n",
    "    train_dataset = RoadsDataset(x_train_dir, y_train_dir,\n",
    "                                 class_rgb_values=CLASSES_RGB, augmentation=transform_train(), preprocessing=prepare_to_network())\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=NUM_WORKERS,\n",
    "    )\n",
    "\n",
    "    # ПАРАМЕТРЫ ДЛЯ ОБУЧЕНИЯ\n",
    "    criterion = get_function_loss(param['loss'])\n",
    "    optimizer = get_optimizer(\n",
    "        param['optimizer'], model, LEARNING_RATE, weight_decay=param['weight_decay'])\n",
    "    scheduler = get_scheduler(param['scheduler'], optimizer)\n",
    "\n",
    "    # ПЕРЕМЕННЫЕ ДЛЯ ГРАФИКОВ\n",
    "    len_steps = len(train_dataloader)\n",
    "\n",
    "    # ОБУЧЕНИЕ\n",
    "    pbar = tqdm(range(EPOCHS))\n",
    "    for epoch in range(EPOCHS):\n",
    "        for i, param_group in enumerate(optimizer.param_groups):\n",
    "            BOARD.add_scalar('learning rate', float(param_group['lr']), epoch)\n",
    "\n",
    "        train_loss = train_step(\n",
    "            model, criterion, optimizer, train_dataloader, epoch, EPOCHS)\n",
    "        valid_loss, metric_iou, metric_dice, metric_acc = valid_step(\n",
    "            model, criterion, valid_dataloader, epoch, BOARD)\n",
    "\n",
    "        scheduler.step(valid_loss)\n",
    "\n",
    "        BOARD.add_scalar('loss_valid', valid_loss, epoch)\n",
    "        BOARD.add_scalar('loss_train', train_loss, epoch)\n",
    "\n",
    "        BOARD.add_scalar('metric_iou', metric_iou, epoch)\n",
    "        BOARD.add_scalar('metric_dice', metric_dice, epoch)\n",
    "        BOARD.add_scalar('metric_acc', metric_acc, epoch)\n",
    "\n",
    "        pbar.update()\n",
    "        pbar.set_description(\n",
    "            f'{param[\"metric\"]}: {get_metric(param[\"metric\"], metric_iou, metric_acc, metric_dice):.2f}  | train/valid loss: {train_loss:.4f}/{valid_loss:.4f}')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(test_dataloader):\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            BOARD.add_figure('test_sample', create_image_plot(\n",
    "                origin=images[0].cpu().numpy().transpose(2, 1, 0),\n",
    "                true=colour_code_segmentation(reverse_one_hot(\n",
    "                    labels[0].cpu().numpy().transpose(2, 1, 0)), CLASSES_RGB),\n",
    "                pred=colour_code_segmentation(reverse_one_hot(\n",
    "                    output[0].cpu().numpy().transpose(2, 1, 0)), CLASSES_RGB)),\n",
    "                i)\n",
    "    BOARD.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
