{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Импорты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# массивы, рандом\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# файлы\n",
    "from os.path import join as pjoin\n",
    "import os\n",
    "import json\n",
    "\n",
    "# модельки\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "# torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# аугментация\n",
    "import albumentations as album\n",
    "\n",
    "# отображение\n",
    "# from tqdm import tqdm\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torchinfo\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from board import uniqufy_path, create_image_plot\n",
    "\n",
    "# метрики\n",
    "from torchmetrics.classification import BinaryJaccardIndex\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# оптимизаторы, изменение lr\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# лос\n",
    "from segmentation_models_pytorch.losses import LovaszLoss, DiceLoss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Константы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './../../roads_dataset_cropped/tiff/'\n",
    "DATA_CLASSES = \"./../../roads_dataset_cropped/label_class_dict.csv\"\n",
    "\n",
    "MEAN_IMAGE_TRANSFORM = [0.4363, 0.4328, 0.3291]\n",
    "MEAN_IMAGE_STD = [0.2129, 0.2075, 0.2038]\n",
    "SIZE_IMAGE = 256\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 15\n",
    "EPOCHS = 15\n",
    "\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_dir = os.path.join(DATA_DIR, 'train')\n",
    "y_train_dir = os.path.join(DATA_DIR, 'train_labels')\n",
    "\n",
    "x_valid_dir = os.path.join(DATA_DIR, 'val')\n",
    "y_valid_dir = os.path.join(DATA_DIR, 'val_labels')\n",
    "\n",
    "x_test_dir = os.path.join(DATA_DIR, 'test')\n",
    "y_test_dir = os.path.join(DATA_DIR, 'test_labels')\n",
    "\n",
    "class_dict = pd.read_csv(DATA_CLASSES)\n",
    "CLASSES = class_dict['name'].tolist()\n",
    "CLASSES_RGB = class_dict[['r','g','b']].values.tolist()\n",
    "\n",
    "print('Классы: ', CLASSES)\n",
    "print('Классы RGB значений: ', CLASSES_RGB)\n",
    "\n",
    "with open('parametrs.json', 'r', encoding='utf-8') as f:\n",
    "    PARAMETERS = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ОТОБРАЖЕНИЕ\n",
    "def print_image(**images):\n",
    "    n_images = len(images)\n",
    "    plt.figure(figsize=(16, 4))\n",
    "    for idx, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n_images, idx + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.title(name.replace('_', ' ').title(), fontsize=20)\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "# ПРЕОБРАЗОВАНИЯ\n",
    "def one_hot_encode(label, label_values):\n",
    "    semantic_map = []\n",
    "    for colour in label_values:\n",
    "        equality = np.equal(label, colour)\n",
    "        class_map = np.all(equality, axis=-1)\n",
    "        semantic_map.append(class_map)\n",
    "    semantic_map = np.stack(semantic_map, axis=-1)\n",
    "    return semantic_map\n",
    "\n",
    "\n",
    "def reverse_one_hot(image):\n",
    "    x = np.argmax(image, axis=-1)\n",
    "    return x\n",
    "\n",
    "def colour_code_segmentation(image, label_values):\n",
    "    colour_codes = np.array(label_values)\n",
    "    x = colour_codes[image.astype(int)]\n",
    "    return x\n",
    "\n",
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "def prepare_to_network():\n",
    "    return album.Lambda(image=to_tensor, mask=to_tensor)\n",
    "\n",
    "def transform_train():\n",
    "    train_transform = album.Compose(\n",
    "        [\n",
    "            album.OneOf(\n",
    "                [\n",
    "                    album.HorizontalFlip(p=1),\n",
    "                    album.VerticalFlip(p=1),\n",
    "                    album.RandomRotate90(p=1),\n",
    "                ],\n",
    "                p=0.75,\n",
    "            ),\n",
    "            album.Normalize(mean=MEAN_IMAGE_TRANSFORM,\n",
    "                            std=MEAN_IMAGE_STD, always_apply=True)\n",
    "        ]\n",
    "    )\n",
    "    return train_transform\n",
    "\n",
    "\n",
    "def transform_test():\n",
    "    test_transform = album.Compose([\n",
    "        album.Normalize(mean=MEAN_IMAGE_TRANSFORM,\n",
    "                        std=MEAN_IMAGE_STD, always_apply=True)\n",
    "    ])\n",
    "    return test_transform\n",
    "\n",
    "def calculate_dice(labels, output):\n",
    "    with torch.no_grad():\n",
    "        y_true = reverse_one_hot(\n",
    "            labels[0].cpu().numpy().transpose(2, 1, 0)).flatten()\n",
    "        y_pred = reverse_one_hot(\n",
    "            output[0].cpu().numpy().transpose(2, 1, 0)).flatten()\n",
    "\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "        metric_values = (2*tp)/(2*tp+tn+tp)\n",
    "        return metric_values\n",
    "\n",
    "\n",
    "def train_step(model, criterion, optimizer, dataloader, epoch, epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.\n",
    "\n",
    "    for i, (images, labels) in enumerate(dataloader):\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        output = model(images)\n",
    "        loss = criterion(output, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss\n",
    "\n",
    "    with torch.no_grad():\n",
    "        train_loss = running_loss / len(dataloader)\n",
    "    return train_loss.item()\n",
    "\n",
    "\n",
    "def valid_step(model, criterion, dataloader, epoch, BOARD):\n",
    "    model.eval()\n",
    "    running_loss = 0.\n",
    "\n",
    "    dice = 0\n",
    "    iou = BinaryJaccardIndex(num_classes=2)\n",
    "    iou.to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, labels) in enumerate(dataloader):\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            output = model(images)\n",
    "\n",
    "            iou(output, labels)\n",
    "            dice += calculate_dice(labels, output)\n",
    "\n",
    "            loss = criterion(output, labels)\n",
    "            running_loss += loss\n",
    "\n",
    "            BOARD.add_figure('valid_sample_' + str(i), create_image_plot(\n",
    "                origin=images[0].cpu().numpy().transpose(2, 1, 0),\n",
    "                true=colour_code_segmentation(reverse_one_hot(\n",
    "                    labels[0].cpu().numpy().transpose(2, 1, 0)), CLASSES_RGB),\n",
    "                pred=colour_code_segmentation(reverse_one_hot(\n",
    "                    output[0].cpu().numpy().transpose(2, 1, 0)), CLASSES_RGB)),\n",
    "                epoch)\n",
    "        \n",
    "        metric_dice =  dice / len(dataloader)\n",
    "        valid_loss = running_loss / len(dataloader)\n",
    "        return valid_loss.item(), iou.compute().item(), metric_dice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Классы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoadsDataset(Dataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            images_dir, \n",
    "            masks_dir, \n",
    "            class_rgb_values=None, \n",
    "            augmentation=None, \n",
    "            preprocessing=None,\n",
    "    ):\n",
    "        self.image_paths = [os.path.join(images_dir, image_id) for image_id in sorted(os.listdir(images_dir))]\n",
    "        self.mask_paths = [os.path.join(masks_dir, image_id) for image_id in sorted(os.listdir(masks_dir))]\n",
    "\n",
    "        self.class_rgb_values = class_rgb_values\n",
    "        self.augmentation = augmentation\n",
    "        self.preprocessing = preprocessing\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        image = cv2.cvtColor(cv2.imread(self.image_paths[i]), cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.cvtColor(cv2.imread(self.mask_paths[i]), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        mask = one_hot_encode(mask, self.class_rgb_values).astype('float')\n",
    "        \n",
    "        if self.augmentation:\n",
    "            sample = self.augmentation(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        \n",
    "        if self.preprocessing:\n",
    "            sample = self.preprocessing(image=image, mask=mask)\n",
    "            image, mask = sample['image'], sample['mask']\n",
    "        return image, mask\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загрузка валидации и теста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = RoadsDataset(x_train_dir, y_train_dir,\n",
    "                             class_rgb_values=CLASSES_RGB, augmentation=transform_train(), preprocessing=prepare_to_network())\n",
    "valid_dataset = RoadsDataset(x_valid_dir, y_valid_dir,\n",
    "                             class_rgb_values=CLASSES_RGB, augmentation=transform_test(), preprocessing=prepare_to_network())\n",
    "test_dataset = RoadsDataset(x_test_dir, y_test_dir,\n",
    "                            class_rgb_values=CLASSES_RGB, augmentation=transform_test(), preprocessing=prepare_to_network())\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=4,\n",
    "    num_workers=4,\n",
    ")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=4,\n",
    "    num_workers=4,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ПОДГОТОВКА ОБУЧЕНИЕ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TBpath = uniqufy_path('res/best')\n",
    "BOARD = SummaryWriter(TBpath)\n",
    "\n",
    "# МОДЕЛЬ\n",
    "model = smp.Unet(\n",
    "    encoder_name='resnet50',\n",
    "    encoder_weights='imagenet',\n",
    "    classes=len(CLASSES),\n",
    "    activation=nn.ReLU)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "# ПАРАМЕТРЫ ДЛЯ ОБУЧЕНИЯ\n",
    "criterion = DiceLoss(mode='binary')\n",
    "optimizer = torch.optim.Adam(\n",
    "    params=model.parameters(), lr=0.001) # weight_decay=0.000001\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min',\n",
    "                                patience=2, threshold=1e-3,\n",
    "                                cooldown=1, factor=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # ОБУЧЕНИЕ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ПЕРЕМЕННЫЕ ДЛЯ ГРАФИКОВ\n",
    "len_steps = len(train_dataloader)\n",
    "\n",
    "pbar = tqdm(range(EPOCHS))\n",
    "for epoch in range(EPOCHS):\n",
    "    for i, param_group in enumerate(optimizer.param_groups):\n",
    "        BOARD.add_scalar('learning rate', float(param_group['lr']), epoch)\n",
    "\n",
    "    train_loss = train_step(\n",
    "        model, criterion, optimizer, train_dataloader, epoch, EPOCHS)\n",
    "    valid_loss, metric_iou, dice = valid_step(\n",
    "        model, criterion, valid_dataloader, epoch, BOARD)\n",
    "\n",
    "    scheduler.step(valid_loss)\n",
    "\n",
    "    BOARD.add_scalar('loss_valid', valid_loss, epoch)\n",
    "    BOARD.add_scalar('loss_train', train_loss, epoch)\n",
    "\n",
    "    BOARD.add_scalar('metric_iou', metric_iou, epoch)\n",
    "    BOARD.add_scalar('metric_dice', (1 - valid_loss), epoch)\n",
    "    \n",
    "    torch.save(model.state_dict(), f'./model_{i}.pth')\n",
    "    \n",
    "    pbar.update()\n",
    "    pbar.set_description(\n",
    "        f'iou: {metric_iou:.2f} dice: {dice:.2f}  | train/valid loss: {train_loss:.4f}/{valid_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou = BinaryJaccardIndex(num_classes=2)\n",
    "iou.to(DEVICE)\n",
    "dice = 0\n",
    "\n",
    "running_loss = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (images, labels) in enumerate(test_dataloader):\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        output = model(images)\n",
    "\n",
    "        dice += calculate_dice(labels, output)\n",
    "        iou.update(output, labels)\n",
    "\n",
    "        BOARD.add_figure('test_sample', create_image_plot(\n",
    "            origin=images[0].cpu().numpy().transpose(2, 1, 0),\n",
    "            true=colour_code_segmentation(reverse_one_hot(\n",
    "                labels[0].cpu().numpy().transpose(2, 1, 0)), CLASSES_RGB),\n",
    "            pred=colour_code_segmentation(reverse_one_hot(\n",
    "                output[0].cpu().numpy().transpose(2, 1, 0)), CLASSES_RGB)),\n",
    "            i)\n",
    "\n",
    "        pbar.update()\n",
    "\n",
    "    metric_dice =  dice / len(test_dataloader)\n",
    "    print(metric_dice)\n",
    "    print(f'dice: {metric_dice}')\n",
    "    print(f'iou: {iou.compute().item()}')\n",
    "\n",
    "BOARD.close()\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
