{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join as pjoin\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import torchmetrics.classification as metrics\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torchinfo\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from additonFunc import uniqufy_path, create_image_plot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Иницилизация ключевых значений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "LEARNING_RATE = 0.01\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "WEIGHT_SAVER = \"last\" # \"all\" / \"nothing\" / \"last\"\n",
    "\n",
    "CLASS_NAMES = ['other', 'road']\n",
    "CLASS_RGB_VALUES = [[0,0,0], [255, 255, 255]]\n",
    "\n",
    "NORMALIZE_MEAN = (0, 0, 0)\n",
    "NORMALIZE_DEVIATIONS = (1, 1, 1)\n",
    "\n",
    "CROP_SIZE = (128, 128)\n",
    "\n",
    "NORMALIZE_MEAN_IMG = [0.4295, 0.4325, 0.3961]\n",
    "NORMALIZE_DEVIATIONS_IMG = [0.2267, 0.2192, 0.2240]\n",
    "\n",
    "NUM_WORKERS = 0\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# TBwriter = SummaryWriter(uniqufy_path(\"TB_cache\\\\roads\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(x, **kwargs):\n",
    "    return x.transpose(2, 0, 1).astype('float32')\n",
    "\n",
    "prepare_to_network = A.Lambda(image=to_tensor, mask=to_tensor)\n",
    "\n",
    "train_transform= A.Compose(\n",
    "    [\n",
    "        A.RandomCrop(*CROP_SIZE, always_apply=True),\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.HorizontalFlip(p=1),\n",
    "                A.VerticalFlip(p=1),\n",
    "                A.RandomRotate90(p=1),\n",
    "            ],\n",
    "            p=0.75,\n",
    "        ),\n",
    "        A.Normalize(mean=NORMALIZE_MEAN_IMG, std=NORMALIZE_DEVIATIONS_IMG, always_apply=True)\n",
    "    ]\n",
    ")\n",
    "\n",
    "valid_transform = A.Compose(\n",
    "    [\n",
    "        A.PadIfNeeded(min_height=1536, min_width=1536, always_apply=True, border_mode=cv2.BORDER_CONSTANT),\n",
    "        A.Normalize(mean=NORMALIZE_MEAN, std=NORMALIZE_DEVIATIONS),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(label, label_values):\n",
    "    semantic_map = []\n",
    "    for colour in label_values:\n",
    "        equality = np.equal(label, colour)\n",
    "        class_map = np.all(equality, axis = -1)\n",
    "        semantic_map.append(class_map)\n",
    "    semantic_map = np.stack(semantic_map, axis=-1)\n",
    "    return semantic_map\n",
    "\n",
    "def reverse_one_hot(image):\n",
    "    x = np.argmax(image, axis = -1)\n",
    "    return x\n",
    "\n",
    "def colour_code_segmentation(image, label_values):\n",
    "    colour_codes = np.array(label_values)\n",
    "    x = colour_codes[image.astype(int)]\n",
    "    return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset's\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoadsDataset(Dataset):\n",
    "    def __init__(self, values_dir, labels_dir, class_rgb_values=None, transform=None, readyToNetwork=None):\n",
    "        self.values_dir = values_dir\n",
    "        self.labels_dir = labels_dir\n",
    "        \n",
    "        self.images = [pjoin(self.values_dir, filename) for filename in sorted(os.listdir(self.values_dir))]\n",
    "        self.labels = [pjoin(self.labels_dir, filename) for filename in sorted(os.listdir(self.labels_dir))]\n",
    "        \n",
    "        self.class_rgb_values = class_rgb_values\n",
    "        self.transform = transform\n",
    "        self.readyToNetwork = readyToNetwork\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_path = self.images[index]\n",
    "        label_path = self.labels[index]\n",
    "\n",
    "        image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "        label = cv2.cvtColor(cv2.imread(label_path), cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "\n",
    "        label = one_hot_encode(label, self.class_rgb_values).astype('float')\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(image=image, mask=label)\n",
    "            image, label = sample['image'], sample['mask']\n",
    "        if self.readyToNetwork:\n",
    "            sample = self.readyToNetwork(image=image, mask=label)\n",
    "            image, label = sample['image'], sample['mask']\n",
    "        return image, label"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader's "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 10 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 46\u001b[0m\n\u001b[0;32m     41\u001b[0m random_idx \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39mlen\u001b[39m(train_dataset)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     42\u001b[0m image, mask \u001b[39m=\u001b[39m train_dataset[random_idx]\n\u001b[0;32m     44\u001b[0m visualize(\n\u001b[0;32m     45\u001b[0m     original_image \u001b[39m=\u001b[39m image,\n\u001b[1;32m---> 46\u001b[0m     ground_truth_mask \u001b[39m=\u001b[39m colour_code_segmentation(reverse_one_hot(mask), CLASS_RGB_VALUES),\n\u001b[0;32m     47\u001b[0m     one_hot_encoded_mask \u001b[39m=\u001b[39m reverse_one_hot(mask)\n\u001b[0;32m     48\u001b[0m )\n",
      "Cell \u001b[1;32mIn[5], line 16\u001b[0m, in \u001b[0;36mcolour_code_segmentation\u001b[1;34m(image, label_values)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcolour_code_segmentation\u001b[39m(image, label_values):\n\u001b[0;32m     15\u001b[0m     colour_codes \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(label_values)\n\u001b[1;32m---> 16\u001b[0m     x \u001b[39m=\u001b[39m colour_codes[image\u001b[39m.\u001b[39;49mastype(\u001b[39mint\u001b[39;49m)]\n\u001b[0;32m     17\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "\u001b[1;31mIndexError\u001b[0m: index 10 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "train_dataset = RoadsDataset(\"../Segmentation/data\\\\tiff\\\\train\", \"../Segmentation/data\\\\tiff\\\\train_labels\",\n",
    "                       class_rgb_values=CLASS_RGB_VALUES, transform=train_transform, readyToNetwork=prepare_to_network)\n",
    "# valid_dataset = RoadsDataset(\"../Segmentation/data\\\\tiff\\\\val\", \"../Segmentation/data\\\\tiff\\\\val_labels\",\n",
    "#                        class_rgb_values=CLASS_RGB_VALUES, transform=valid_transform, readyToNetwork=prepare_to_network)\n",
    "# test_dataset = RoadsDataset(\"../Segmentation/data\\\\tiff\\\\test\", \"../Segmentation/data\\\\tiff\\\\test_labels\",\n",
    "#                        class_rgb_values=CLASS_RGB_VALUES, transform=valid_transform, readyToNetwork=prepare_to_network)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    ")\n",
    "# valid_dataloader = DataLoader(\n",
    "#     valid_dataset,\n",
    "#     batch_size=1,\n",
    "#     shuffle=True,\n",
    "#     num_workers=0,\n",
    "# )\n",
    "# test_dataloader = DataLoader(\n",
    "#     test_dataset,\n",
    "#     batch_size=1,\n",
    "#     shuffle=True,\n",
    "#     num_workers=0,\n",
    "# )\n",
    "import random, tqdm\n",
    "\n",
    "def visualize(**images):\n",
    "    n_images = len(images)\n",
    "    plt.figure(figsize=(16, 4))\n",
    "    for idx, (name, image) in enumerate(images.items()):\n",
    "        plt.subplot(1, n_images, idx + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        # get title from the parameter names\n",
    "        plt.title(name.replace('_', ' ').title(), fontsize=20)\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "for i in range(3):\n",
    "    random_idx = random.randint(0, len(train_dataset)-1)\n",
    "    image, mask = train_dataset[random_idx]\n",
    "    \n",
    "    visualize(\n",
    "        original_image = image,\n",
    "        ground_truth_mask = colour_code_segmentation(reverse_one_hot(mask), CLASS_RGB_VALUES),\n",
    "        one_hot_encoded_mask = reverse_one_hot(mask)\n",
    "    )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================================================================================================================================================================================\n",
      "Layer (type (var_name))                            Input Shape               Output Shape              Param #                   Param %                   Kernel Shape              Mult-Adds                 Trainable\n",
      "=================================================================================================================================================================================================================================\n",
      "Unet (Unet)                                        [32, 3, 128, 128]         [32, 2, 128, 128]         --                             --                   --                        --                        True\n",
      "├─MobileNetV2Encoder (encoder)                     [32, 3, 128, 128]         [32, 3, 128, 128]         --                             --                   --                        --                        True\n",
      "│    └─Sequential (features)                       --                        --                        --                             --                   --                        --                        True\n",
      "│    │    └─Conv2dNormActivation (0)               [32, 3, 128, 128]         [32, 32, 64, 64]          928                         0.01%                   --                        113,248,256               True\n",
      "│    │    └─InvertedResidual (1)                   [32, 32, 64, 64]          [32, 16, 64, 64]          896                         0.01%                   --                        104,860,672               True\n",
      "│    │    └─InvertedResidual (2)                   [32, 16, 64, 64]          [32, 24, 32, 32]          5,136                       0.08%                   --                        305,149,440               True\n",
      "│    │    └─InvertedResidual (3)                   [32, 24, 32, 32]          [32, 24, 32, 32]          8,832                       0.13%                   --                        268,979,712               True\n",
      "│    │    └─InvertedResidual (4)                   [32, 24, 32, 32]          [32, 32, 16, 16]          10,000                      0.15%                   --                        161,632,256               True\n",
      "│    │    └─InvertedResidual (5)                   [32, 32, 16, 16]          [32, 32, 16, 16]          14,848                      0.22%                   --                        114,845,696               True\n",
      "│    │    └─InvertedResidual (6)                   [32, 32, 16, 16]          [32, 32, 16, 16]          14,848                      0.22%                   --                        114,845,696               True\n",
      "│    │    └─InvertedResidual (7)                   [32, 32, 16, 16]          [32, 64, 8, 8]            21,056                      0.32%                   --                        79,065,088                True\n",
      "│    │    └─InvertedResidual (8)                   [32, 64, 8, 8]            [32, 64, 8, 8]            54,272                      0.82%                   --                        107,794,432               True\n",
      "│    │    └─InvertedResidual (9)                   [32, 64, 8, 8]            [32, 64, 8, 8]            54,272                      0.82%                   --                        107,794,432               True\n",
      "│    │    └─InvertedResidual (10)                  [32, 64, 8, 8]            [32, 64, 8, 8]            54,272                      0.82%                   --                        107,794,432               True\n",
      "│    │    └─InvertedResidual (11)                  [32, 64, 8, 8]            [32, 96, 8, 8]            66,624                      1.01%                   --                        132,962,304               True\n",
      "│    │    └─InvertedResidual (12)                  [32, 96, 8, 8]            [32, 96, 8, 8]            118,272                     1.78%                   --                        237,189,120               True\n",
      "│    │    └─InvertedResidual (13)                  [32, 96, 8, 8]            [32, 96, 8, 8]            118,272                     1.78%                   --                        237,189,120               True\n",
      "│    │    └─InvertedResidual (14)                  [32, 96, 8, 8]            [32, 160, 4, 4]           155,264                     2.34%                   --                        163,170,304               True\n",
      "│    │    └─InvertedResidual (15)                  [32, 160, 4, 4]           [32, 160, 4, 4]           320,000                     4.83%                   --                        161,843,200               True\n",
      "│    │    └─InvertedResidual (16)                  [32, 160, 4, 4]           [32, 160, 4, 4]           320,000                     4.83%                   --                        161,843,200               True\n",
      "│    │    └─InvertedResidual (17)                  [32, 160, 4, 4]           [32, 320, 4, 4]           473,920                     7.15%                   --                        240,496,640               True\n",
      "│    │    └─Conv2dNormActivation (18)              [32, 320, 4, 4]           [32, 1280, 4, 4]          412,160                     6.22%                   --                        209,797,120               True\n",
      "├─UnetDecoder (decoder)                            [32, 3, 128, 128]         [32, 16, 128, 128]        --                             --                   --                        --                        True\n",
      "│    └─Identity (center)                           [32, 1280, 4, 4]          [32, 1280, 4, 4]          --                             --                   --                        --                        --\n",
      "│    └─ModuleList (blocks)                         --                        --                        --                             --                   --                        --                        True\n",
      "│    │    └─DecoderBlock (0)                       [32, 1280, 4, 4]          [32, 256, 8, 8]           3,761,152                  56.74%                   --                        7,700,774,912             True\n",
      "│    │    └─DecoderBlock (1)                       [32, 256, 8, 8]           [32, 128, 16, 16]         479,744                     7.24%                   --                        3,925,884,928             True\n",
      "│    │    └─DecoderBlock (2)                       [32, 128, 16, 16]         [32, 64, 32, 32]          124,672                     1.88%                   --                        4,076,871,680             True\n",
      "│    │    └─DecoderBlock (3)                       [32, 64, 32, 32]          [32, 32, 64, 64]          32,384                      0.49%                   --                        4,227,862,528             True\n",
      "│    │    └─DecoderBlock (4)                       [32, 32, 64, 64]          [32, 16, 128, 128]        6,976                       0.11%                   --                        3,623,880,704             True\n",
      "├─SegmentationHead (segmentation_head)             [32, 16, 128, 128]        [32, 2, 128, 128]         --                             --                   --                        --                        True\n",
      "│    └─Conv2d (0)                                  [32, 16, 128, 128]        [32, 2, 128, 128]         290                         0.00%                   [3, 3]                    152,043,520               True\n",
      "│    └─Identity (1)                                [32, 2, 128, 128]         [32, 2, 128, 128]         --                             --                   --                        --                        --\n",
      "│    └─Activation (2)                              [32, 2, 128, 128]         [32, 2, 128, 128]         --                             --                   --                        --                        --\n",
      "│    │    └─Sigmoid (activation)                   [32, 2, 128, 128]         [32, 2, 128, 128]         --                             --                   --                        --                        --\n",
      "=================================================================================================================================================================================================================================\n",
      "Total params: 6,629,090\n",
      "Trainable params: 6,629,090\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (G): 26.84\n",
      "=================================================================================================================================================================================================================================\n",
      "Input size (MB): 6.29\n",
      "Forward/backward pass size (MB): 1644.95\n",
      "Params size (MB): 26.52\n",
      "Estimated Total Size (MB): 1677.76\n",
      "=================================================================================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "ENCODER = 'mobilenet_v2'\n",
    "CLASSES = CLASS_NAMES\n",
    "ACTIVATION = \"sigmoid\"\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=ENCODER, \n",
    "    classes=len(CLASSES), \n",
    "    activation=ACTIVATION,\n",
    ")\n",
    "\n",
    "# images, _ = next(iter(test_dataloader))\n",
    "# TBwriter.add_graph(model, images)\n",
    "\n",
    "# torch.onnx.export(model,\n",
    "#                   images,\n",
    "#                   \"model.onnx\")\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "print(model_sum := torchinfo.summary(model, input_size=(BATCH_SIZE, 3, *CROP_SIZE), row_settings=[\"var_names\"], verbose=0, col_names=[\n",
    "      \"input_size\", \"output_size\", \"num_params\", \"params_percent\", \"kernel_size\", \"mult_adds\", \"trainable\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = smp.losses.DiceLoss(mode='binary')\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=0.0001)\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addTuples(a1 : tuple, a2 : tuple):\n",
    "    for i in range(len(a1)):\n",
    "        a1[i] += a2[i]\n",
    "    return a1\n",
    "    \n",
    "\n",
    "def train_step(net, criterion, optimizer, dataloader, epoch : int = None):\n",
    "    net.train()\n",
    "    running_loss = 0.\n",
    "    for images, labels in dataloader:\n",
    "        images = images.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output = net(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss\n",
    "\n",
    "    with torch.no_grad():\n",
    "        train_loss = running_loss / len(train_dataloader)\n",
    "    return train_loss.item()\n",
    "\n",
    "def valid_step(net, criterion, dataloader, epoch : int = None):\n",
    "    net.eval()\n",
    "    running_loss = 0.\n",
    "    IoU = metrics.BinaryJaccardIndex()\n",
    "    IoU.to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "\n",
    "            output = net(images)\n",
    "\n",
    "            IoU(output, labels)\n",
    "            loss = criterion(output, labels)\n",
    "            running_loss += loss\n",
    "\n",
    "        valid_loss = running_loss / len(valid_dataloader)\n",
    "\n",
    "        return valid_loss.item(), IoU.compute().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IoU: 0.83  | train/valid loss: 0.3545/0.3758:  50%|█████     | 5/10 [17:40<17:43, 212.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved weights with IoU: 0.83 | loss: 0.3758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IoU: 0.83  | train/valid loss: 0.3545/0.3758:  50%|█████     | 5/10 [19:12<19:12, 230.58s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m trained \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m (pbar \u001b[39m:=\u001b[39m tqdm(\u001b[39mrange\u001b[39m(EPOCHS))):\n\u001b[1;32m----> 5\u001b[0m     train_loss \u001b[39m=\u001b[39m train_step(model, loss, optimizer, train_dataloader, epoch)\n\u001b[0;32m      6\u001b[0m     valid_loss, iou_score \u001b[39m=\u001b[39m valid_step(model, loss, valid_dataloader, epoch)\n\u001b[0;32m      8\u001b[0m     \u001b[39mif\u001b[39;00m WEIGHT_SAVER \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnothing\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m valid_loss \u001b[39m<\u001b[39m best_loss \u001b[39mand\u001b[39;00m epoch \u001b[39m>\u001b[39m \u001b[39m3\u001b[39m:\n",
      "Cell \u001b[1;32mIn[19], line 10\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(net, criterion, optimizer, dataloader, epoch)\u001b[0m\n\u001b[0;32m      8\u001b[0m net\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m      9\u001b[0m running_loss \u001b[39m=\u001b[39m \u001b[39m0.\u001b[39m\n\u001b[1;32m---> 10\u001b[0m \u001b[39mfor\u001b[39;00m images, labels \u001b[39min\u001b[39;00m dataloader:\n\u001b[0;32m     11\u001b[0m     images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(DEVICE)\n\u001b[0;32m     12\u001b[0m     labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(DEVICE)\n",
      "File \u001b[1;32mc:\\Users\\emely\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\emely\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\emely\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\emely\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[15], line 24\u001b[0m, in \u001b[0;36mRoadsDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     20\u001b[0m image \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(cv2\u001b[39m.\u001b[39mimread(image_path), cv2\u001b[39m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[0;32m     21\u001b[0m label \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(cv2\u001b[39m.\u001b[39mimread(label_path), cv2\u001b[39m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m---> 24\u001b[0m label \u001b[39m=\u001b[39m one_hot_encode(label, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_rgb_values)\u001b[39m.\u001b[39mastype(\u001b[39m'\u001b[39m\u001b[39mfloat\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform:\n\u001b[0;32m     27\u001b[0m     sample \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform(image\u001b[39m=\u001b[39mimage, mask\u001b[39m=\u001b[39mlabel)\n",
      "Cell \u001b[1;32mIn[14], line 4\u001b[0m, in \u001b[0;36mone_hot_encode\u001b[1;34m(label, label_values)\u001b[0m\n\u001b[0;32m      2\u001b[0m semantic_map \u001b[39m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m colour \u001b[39min\u001b[39;00m label_values:\n\u001b[1;32m----> 4\u001b[0m     equality \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mequal(label, colour)\n\u001b[0;32m      5\u001b[0m     class_map \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mall(equality, axis \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      6\u001b[0m     semantic_map\u001b[39m.\u001b[39mappend(class_map)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_loss = 10000\n",
    "trained = True\n",
    "\n",
    "for epoch in (pbar := tqdm(range(EPOCHS))):\n",
    "    train_loss = train_step(model, loss, optimizer, train_dataloader, epoch)\n",
    "    valid_loss, iou_score = valid_step(model, loss, valid_dataloader, epoch)\n",
    "\n",
    "    if WEIGHT_SAVER != \"nothing\" and valid_loss < best_loss and epoch > 3:\n",
    "        best_loss = valid_loss\n",
    "\n",
    "        print(f\"Saved weights with IoU: {iou_score:.2f} | loss: {valid_loss:.4f}\")\n",
    "\n",
    "    pbar.set_description(\n",
    "        f'IoU: {iou_score:.2f}  | train/valid loss: {train_loss:.4f}/{valid_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBwriter.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
